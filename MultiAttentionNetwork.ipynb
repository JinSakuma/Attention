{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import resnet34, resnet18\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.ImageFolder(root='./data/train', transform=train_preprocess)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True,\n",
    "#                                            num_workers=4\n",
    "                                          ) \n",
    "\n",
    "dataset_test = torchvision.datasets.ImageFolder(root='./data/test', transform=test_preprocess)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True,\n",
    "#                                           num_workers=4\n",
    "                                         ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionNetwork(nn.Module):\n",
    "    def __init__(self, num_classes, num_masks=2):\n",
    "        super().__init__()\n",
    "\n",
    "        base_model = resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*[layer for layer in base_model.children()][:-2])\n",
    "        self.attn_conv = nn.Conv2d(512, num_masks, 1, bias=False)\n",
    "        nn.init.xavier_uniform_(self.attn_conv.weight)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * num_masks, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        self.mask_ = None\n",
    "        self.num_masks = num_masks\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        \n",
    "        attn = torch.sigmoid(self.attn_conv(x))  # [B, M, H, W]\n",
    "        B, _, H, W = attn.shape\n",
    "        self.mask_ = attn\n",
    "        \n",
    "        x = x.reshape(B, 1, 512, H, W)\n",
    "        attn = attn.reshape(B, self.num_masks, 1, H, W)\n",
    "        \n",
    "        x = x * attn  # [B, M, 512, H, W]\n",
    "        x = x.reshape(B * self.num_masks, -1, H, W)  # [BM, 512, H, W]\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # [BM, 512, 1, 1]\n",
    "        \n",
    "        x = x.reshape(B, -1)\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "    def divergence_loss(self):\n",
    "        mask = self.mask_  # [B, M, H, W]\n",
    "        B, M, H, W = mask.shape\n",
    "        device = mask.device\n",
    "        \n",
    "        flatten_mask = mask.reshape(B, M, -1)\n",
    "        diag = 1 - torch.eye(M).unsqueeze(0).to(device)  # [1, M, M]\n",
    "        \n",
    "        max_val, _ = flatten_mask.max(dim=2, keepdim=True)\n",
    "        flatten_mask = flatten_mask / (max_val + 1e-2)\n",
    "        \n",
    "        div_loss = torch.bmm(flatten_mask, flatten_mask.transpose(1, 2)) * diag  # [B, M, M] x [1, M, M]\n",
    "        return (div_loss.view(-1) ** 2).mean()\n",
    "    \n",
    "    def make_cam(self, img, mask):\n",
    "        cam = cv2.resize(mask, (224, 224))\n",
    "        heatmap = (cam - np.min(cam))/(np.max(cam) - np.min(cam))    # 私の自作モデルではこちらを使用\n",
    "        image = img.transpose(1, 2, 0)\n",
    "        image -= np.min(image)\n",
    "        image = np.uint8(255*image)\n",
    "        image = np.minimum(image, 255)\n",
    "        cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "        cam = np.float32(cam) + np.float32(image)\n",
    "        cam = 255 * cam / np.max(cam)\n",
    "        return np.uint8(cam)[:,:,::-1]        \n",
    "    \n",
    "    def save_attention_mask(self, x, path, head=4):\n",
    "        B = x.shape[0]\n",
    "        self.forward(x)\n",
    "        x = x.cpu() * torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n",
    "        x = x + torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n",
    "        fig, axs = plt.subplots(min(B, head), self.num_masks*2+1, figsize=(16, 2 * min(B, head)), squeeze=False)\n",
    "        plt.axis('off')\n",
    "        mask = self.mask_.detach().cpu()\n",
    "        for i in range(min(B, head)):\n",
    "            axs[i, 0].imshow(x[i].permute(1, 2, 0))\n",
    "            for j in range(0, self.num_masks):\n",
    "                axs[i, j*2+1].imshow(mask[i, j], vmin = 0, vmax = 1)\n",
    "                cam = self.make_cam(x[i].numpy(), mask[i, j].numpy())\n",
    "                axs[i, j*2+2].imshow(cam, vmin = 0, vmax = 1)\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "gpu_flag = torch.cuda.is_available()\n",
    "print(gpu_flag)\n",
    "if gpu_flag:\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, lambda_divergence):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    for X, y in tqdm(loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        clf = model(X)\n",
    "        loss = clf_loss_func(clf, y)\n",
    "        loss += lambda_divergence * model.divergence_loss()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        predict = clf.argmax(dim=1)\n",
    "        correct += (predict == y.data).sum()\n",
    "        total += len(y)\n",
    "    \n",
    "    return np.mean(losses), float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, loader):\n",
    "    model.eval()\n",
    "  \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            clf = model(X)\n",
    "            loss = clf_loss_func(clf, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            predict = clf.argmax(dim=1)\n",
    "            correct += (predict == y).sum().item()\n",
    "            total += len(y)\n",
    "            \n",
    "    return np.mean(losses), float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_attention_model = MultiAttentionNetwork(2)\n",
    "multi_attention_model = multi_attention_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.64it/s]\n",
      "100%|██████████| 16/16 [00:05<00:00,  3.13it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "train loss: 0.49, train acc: 78.40%\n",
      "val loss: 0.14, val acc: 96.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.65it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.40it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "train loss: 0.13, train acc: 96.92%\n",
      "val loss: 0.07, val acc: 97.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.69it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.31it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "train loss: 0.07, train acc: 98.20%\n",
      "val loss: 0.05, val acc: 98.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.72it/s]\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.21it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "train loss: 0.04, train acc: 99.00%\n",
      "val loss: 0.05, val acc: 98.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 74/79 [00:27<00:01,  2.77it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(multi_attention_model.parameters(), lr=0.001, momentum=0.9)\n",
    "best_loss = 1e+10\n",
    "lambda_divergence = 5e-04\n",
    "# best_state = None\n",
    "earlystop_counter = 0\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train(multi_attention_model, loader_train, optimizer, lambda_divergence)\n",
    "    val_loss, val_acc = valid(multi_attention_model, loader_test)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "#         best_state = model.cpu().state_dict()\n",
    "        \n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print(\"train loss: {:.2f}, train acc: {:.2f}%\".format(train_loss, train_acc*100.))\n",
    "    print(\"val loss: {:.2f}, val acc: {:.2f}%\".format(val_loss, val_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in loader_test:\n",
    "    img = img.to(device)\n",
    "    mask = multi_attention_model.save_attention_mask(img, os.path.join('.', 'out.png'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
